{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>HAM_0002555</td>\n",
       "      <td>ISIC_0032398</td>\n",
       "      <td>nv</td>\n",
       "      <td>follow_up</td>\n",
       "      <td>55.0</td>\n",
       "      <td>male</td>\n",
       "      <td>trunk</td>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>Melanocytic nevi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>HAM_0005492</td>\n",
       "      <td>ISIC_0032138</td>\n",
       "      <td>df</td>\n",
       "      <td>histo</td>\n",
       "      <td>35.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>Dermatofibroma</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>HAM_0004837</td>\n",
       "      <td>ISIC_0030735</td>\n",
       "      <td>nv</td>\n",
       "      <td>follow_up</td>\n",
       "      <td>50.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>Melanocytic nevi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lesion_id      image_id  dx    dx_type   age     sex     localization  \\\n",
       "3986  HAM_0002555  ISIC_0032398  nv  follow_up  55.0    male            trunk   \n",
       "1129  HAM_0005492  ISIC_0032138  df      histo  35.0  female  lower extremity   \n",
       "5983  HAM_0004837  ISIC_0030735  nv  follow_up  50.0  female  lower extremity   \n",
       "\n",
       "                                                   path         cell_type  \\\n",
       "3986  /Users/csengeszoke/programming/Thesis/Code/Fai...  Melanocytic nevi   \n",
       "1129  /Users/csengeszoke/programming/Thesis/Code/Fai...    Dermatofibroma   \n",
       "5983  /Users/csengeszoke/programming/Thesis/Code/Fai...  Melanocytic nevi   \n",
       "\n",
       "      cell_type_idx  \n",
       "3986              4  \n",
       "1129              3  \n",
       "5983              4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "base_skin_dir = \"/Users/csengeszoke/programming/Thesis/Code/Fairness-Rectification/Data\"\n",
    "\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis-like lesions',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(base_skin_dir, 'HAM10000_metadata.csv')\n",
    "\n",
    "tile_df = pd.read_csv(metadata_path)\n",
    "\n",
    "tile_df['path'] = tile_df['image_id'].map(imageid_path_dict.get)\n",
    "tile_df['cell_type'] = tile_df['dx'].map(lesion_type_dict.get) \n",
    "tile_df['cell_type_idx'] = pd.Categorical(tile_df['cell_type']).codes\n",
    "\n",
    "tile_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/csengeszoke/programming/Thesis/Code/Fai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filepaths labels\n",
       "0  /Users/csengeszoke/programming/Thesis/Code/Fai...      2\n",
       "1  /Users/csengeszoke/programming/Thesis/Code/Fai...      2\n",
       "2  /Users/csengeszoke/programming/Thesis/Code/Fai...      2\n",
       "3  /Users/csengeszoke/programming/Thesis/Code/Fai...      2\n",
       "4  /Users/csengeszoke/programming/Thesis/Code/Fai...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['filepaths'] =tile_df['path']\n",
    "df['labels'] =tile_df['cell_type_idx']\n",
    "df['labels'] =df['labels'].apply(lambda x: str(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "male       0.539790\n",
      "female     0.454518\n",
      "unknown    0.005691\n",
      "Name: count, dtype: float64\n",
      "age\n",
      "45.0    1299\n",
      "50.0    1187\n",
      "55.0    1009\n",
      "40.0     985\n",
      "60.0     803\n",
      "70.0     756\n",
      "35.0     753\n",
      "65.0     731\n",
      "75.0     618\n",
      "30.0     464\n",
      "80.0     404\n",
      "85.0     290\n",
      "25.0     247\n",
      "20.0     169\n",
      "5.0       86\n",
      "15.0      77\n",
      "10.0      41\n",
      "0.0       39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This df will be used for fairness evaluation\n",
    "df2 = pd.DataFrame() \n",
    "\n",
    "df2['filepaths'] = tile_df['path']\n",
    "df2['labels'] = tile_df['cell_type_idx']\n",
    "#df2['labels'] = df['labels'].apply(lambda x: str(x))\n",
    "df2['age'] = tile_df['age']\n",
    "df2['sex'] = tile_df['sex']\n",
    "df2.head()\n",
    "\n",
    "gender_counts = df2['sex'].value_counts()\n",
    "print(gender_counts/len(df2))\n",
    "\n",
    "age_counts = df2['age'].value_counts()\n",
    "print(age_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df length:  6009   test_df length:  2003   valid_df length:  2003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10015"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_df(df, trsize, column):\n",
    "    train_df, temp_df = train_test_split(df, train_size=trsize, shuffle=True, random_state=42, stratify=df[column])\n",
    "    valid_df, test_df= train_test_split(temp_df, train_size=.50, shuffle=True, random_state=42, stratify=temp_df[column])\n",
    "    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df)) \n",
    "    return train_df, valid_df, test_df \n",
    "\n",
    "train_df, valid_df, test_df = split_df(df2, .60, 'labels')\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self,data_frame, transform=None, target_transform=None,batch_size=32):\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame['filepaths'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_frame['filepaths'].iloc[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label =  self.data_frame['labels'].iloc[idx]\n",
    "        age = self.data_frame['age'].iloc[idx]\n",
    "        gender = self.data_frame['sex'].iloc[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, torch.tensor(label, dtype=torch.long),age,gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "train_dataloader = DataLoader(CustomImageDataset(data_frame = train_df, transform=transform),batch_size=32)\n",
    "val_dataloader =  DataLoader(CustomImageDataset(data_frame = valid_df,transform=transform) , batch_size=32)\n",
    "test_dataloader =  DataLoader(CustomImageDataset(data_frame = test_df,transform=transform),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csengeszoke/programming/Thesis/Code/Fairness-Rectification/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/csengeszoke/programming/Thesis/Code/Fairness-Rectification/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/programming/Thesis/Code/Fairness-Rectification/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/Thesis/Code/Fairness-Rectification/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# # Load pre-trained ResNet-18\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 7)  # Adjusting output layer for 7 classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "top_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels,age,gender in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "image_path = train_df.iloc[0]['filepaths']  # Replace with actual column name\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Load image\n",
    "print(type(image))\n",
    "\n",
    "image = transform(image)  # Convert PIL image to Tensor\n",
    "print(type(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
